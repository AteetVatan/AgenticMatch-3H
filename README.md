# AgenticMatch-3H
Agentic image-matching AI prototype built in 3 hours.

app.py ----- Calls the core agents:
Embedding Agent → turns image into vector using CLIP.
Vector Match Agent → searches FAISS for similar images.
Metadata Agent → fetches mood/style data from JSON.
Returns → matching partners and reasons as JSON response.

├── partner_metadata.json
Purpose: Mock database of partner descriptions.
Contents: Style metadata for each partner image.
Fields (for each partner):
name: Brand or partner name.
style_keywords: Tags like "bohemian", "luxury".
mood: "romantic", "warm", "minimalist" etc.
category: E.g., "fashion", "art", "interior".
description: Aesthetic summary used in UI/LLM.
image_path: Relative path to image (optional for frontend).

├── id_to_partner.json
Purpose: Links FAISS internal IDs to your partner IDs (like P001, P002).

Why Needed: FAISS only stores numeric IDs. 
This file maps them back to real metadata keys for later enrichment.


├── partner_index.faiss
Purpose: Serialized FAISS vector index built from partner images.

Contents:

512-dimensional image embeddings from CLIP
Used for fast similarity search
Generated by: generate_embeddings.py script during Hour 1.
artners/
├── images/
│   ├── 001.jpg      ← image of partner with ID 001
│   ├── 002.jpg      ← image of partner with ID 002
│   └── ...
partner_metadata.json  ← full details of each partner
id_to_partner.json     ← index-to-ID lookup (FAISS index → metadata)
partner_index.faiss    ← vector index built from images


